{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XG Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TcCTq7tyUGCs"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class AIVN_XGBoostRegressor():\n",
        "    '''XGBoost from Scratch\n",
        "    '''\n",
        "\n",
        "    def __init__(self, params, random_seed=None):\n",
        "        self.params = defaultdict(lambda: None, params)\n",
        "        self.subsample = self.params['subsample'] \\\n",
        "            if self.params['subsample'] else 1.0\n",
        "        self.learning_rate = self.params['learning_rate'] \\\n",
        "            if self.params['learning_rate'] else 0.3\n",
        "        self.base_prediction = self.params['base_score'] \\\n",
        "            if self.params['base_score'] else 0.5\n",
        "        self.max_depth = self.params['max_depth'] \\\n",
        "            if self.params['max_depth'] else 5\n",
        "        self.rng = np.random.default_rng(seed=random_seed)\n",
        "\n",
        "    def fit(self, X, y, objective, num_boost_round, verbose=False):\n",
        "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
        "        self.boosters = []\n",
        "        for i in range(num_boost_round):\n",
        "            gradients = objective.gradient(y, current_predictions)\n",
        "            hessians = objective.hessian(y, current_predictions)\n",
        "            sample_idxs = None if self.subsample == 1.0 \\\n",
        "                else self.rng.choice(len(y),\n",
        "                                     size=math.floor(self.subsample*len(y)),\n",
        "                                     replace=False)\n",
        "            booster = TreeBooster(X, gradients, hessians,\n",
        "                                  self.params, self.max_depth, sample_idxs)\n",
        "            current_predictions += self.learning_rate * booster.predict(X)\n",
        "            self.boosters.append(booster)\n",
        "            if verbose:\n",
        "                print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.base_prediction + self.learning_rate\n",
        "                * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n",
        "\n",
        "class TreeBooster():\n",
        "\n",
        "    def __init__(self, X, g, h, params, max_depth, idxs=None):\n",
        "        self.params = params\n",
        "        self.max_depth = max_depth\n",
        "        assert self.max_depth >= 0, 'max_depth must be nonnegative'\n",
        "        self.min_child_weight = params['min_child_weight'] \\\n",
        "            if params['min_child_weight'] else 1.0\n",
        "        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0\n",
        "        self.gamma = params['gamma'] if params['gamma'] else 0.0\n",
        "        self.colsample_bynode = params['colsample_bynode'] \\\n",
        "            if params['colsample_bynode'] else 1.0\n",
        "        if isinstance(g, pd.Series): g = g.values\n",
        "        if isinstance(h, pd.Series): h = h.values\n",
        "        if idxs is None: idxs = np.arange(len(g))\n",
        "        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n",
        "        self.n, self.c = len(idxs), X.shape[1]\n",
        "        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n",
        "        self.best_score_so_far = 0.\n",
        "        if self.max_depth > 0:\n",
        "            self._maybe_insert_child_nodes()\n",
        "\n",
        "    def _maybe_insert_child_nodes(self):\n",
        "        for i in range(self.c): self._find_better_split(i)\n",
        "        if self.is_leaf: return\n",
        "        x = self.X.values[self.idxs,self.split_feature_idx]\n",
        "        left_idx = np.nonzero(x <= self.threshold)[0]\n",
        "        right_idx = np.nonzero(x > self.threshold)[0]\n",
        "        self.left = TreeBooster(self.X, self.g, self.h, self.params,\n",
        "                                self.max_depth - 1, self.idxs[left_idx])\n",
        "        self.right = TreeBooster(self.X, self.g, self.h, self.params,\n",
        "                                 self.max_depth - 1, self.idxs[right_idx])\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self): return self.best_score_so_far == 0.\n",
        "\n",
        "    def _find_better_split(self, feature_idx):\n",
        "        x = self.X.values[self.idxs, feature_idx]\n",
        "        g, h = self.g[self.idxs], self.h[self.idxs]\n",
        "        sort_idx = np.argsort(x)\n",
        "        sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n",
        "        sum_g, sum_h = g.sum(), h.sum()\n",
        "        sum_g_right, sum_h_right = sum_g, sum_h\n",
        "        sum_g_left, sum_h_left = 0., 0.\n",
        "\n",
        "        for i in range(0, self.n - 1):\n",
        "            g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n",
        "            sum_g_left += g_i; sum_g_right -= g_i\n",
        "            sum_h_left += h_i; sum_h_right -= h_i\n",
        "            if sum_h_left < self.min_child_weight or x_i == x_i_next:continue\n",
        "            if sum_h_right < self.min_child_weight: break\n",
        "\n",
        "            gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n",
        "                            + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n",
        "                            - (sum_g**2 / (sum_h + self.reg_lambda))\n",
        "                            ) - self.gamma/2 # Eq(7) in the xgboost paper\n",
        "            if gain > self.best_score_so_far:\n",
        "                self.split_feature_idx = feature_idx\n",
        "                self.best_score_so_far = gain\n",
        "                self.threshold = (x_i + x_i_next) / 2\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n",
        "\n",
        "    def _predict_row(self, row):\n",
        "        if self.is_leaf:\n",
        "            return self.value\n",
        "        child = self.left if row[self.split_feature_idx] <= self.threshold \\\n",
        "            else self.right\n",
        "        return child._predict_row(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zxkwHCs0VF0W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SquaredErrorObjective():\n",
        "    def loss(self, y, pred): return np.mean((y - pred)**2)\n",
        "    def gradient(self, y, pred): return pred - y\n",
        "    def hessian(self, y, pred): return np.ones(len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-VtX4xEmHXML"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#READ DATA\n",
        "data = pd.read_csv(\"../sources/advertising.csv\")\n",
        "\n",
        "#X,y\n",
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Wv90gL7SV2TP"
      },
      "outputs": [],
      "source": [
        "# from sklearn.datasets import fetch_california_housing\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "#                                                     random_state=43)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xq6LYVLpVTFD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\vanna\\AppData\\Local\\Temp\\ipykernel_23104\\2211069773.py:110: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  child = self.left if row[self.split_feature_idx] <= self.threshold \\\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 5,\n",
        "    'subsample': 0.8,\n",
        "    'reg_lambda': 1.5,\n",
        "    'gamma': 0.0,\n",
        "    'min_child_weight': 25,\n",
        "    'base_score': 0.0,\n",
        "    'tree_method': 'exact',\n",
        "}\n",
        "num_boost_round = 50\n",
        "\n",
        "# train the from-scratch XGBoost model\n",
        "model_scratch = AIVN_XGBoostRegressor(params, random_seed=42)\n",
        "model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAInh4MqV9U_",
        "outputId": "263fd52d-a664-4d17-fefc-71bcf824cc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our implementation: 1.7017064684759806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\vanna\\AppData\\Local\\Temp\\ipykernel_23104\\2211069773.py:110: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  child = self.left if row[self.split_feature_idx] <= self.threshold \\\n"
          ]
        }
      ],
      "source": [
        "pred_scratch = model_scratch.predict(X_test)\n",
        "print(f'Our implementation: {SquaredErrorObjective().loss(y_test, pred_scratch)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMCb5pgfPAO8",
        "outputId": "5e1147a0-b645-481e-a2ca-b6fcfdcd2a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBRegressor Lib: 1.0949672299526045\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred_scratch = model.predict(X_test)\n",
        "print(f'XGBRegressor Lib: {SquaredErrorObjective().loss(y_test, pred_scratch)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
